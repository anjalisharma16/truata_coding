{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###This code is executed on Google Colab due to resource constraints. The necessary artifacts for this code are stored in a Git"
      ],
      "metadata": {
        "id": "0kS2dz9b9S2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install necessary library using pip\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA6kTiWP3KqQ",
        "outputId": "cbddb5a1-91f7-488d-9e4a-3ca62e4a2455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=dbff77dc45fffb35734c08d53ca00e68feff3a553b3f7b08af9366654fa2d68a\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 1: Spark RDD API \n"
      ],
      "metadata": {
        "id": "krHcp2tu8Spr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 1: Solution"
      ],
      "metadata": {
        "id": "XzCIDlYv2ob_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQczm60m2qHb",
        "outputId": "14b8c2cf-20dd-4173-8de4-10ff339cac97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------------+--------------+--------------------+\n",
            "|             _c0|                _c1|           _c2|                 _c3|\n",
            "+----------------+-------------------+--------------+--------------------+\n",
            "|    citrus fruit|semi-finished bread|     margarine|         ready soups|\n",
            "|  tropical fruit|             yogurt|        coffee|                null|\n",
            "|      whole milk|               null|          null|                null|\n",
            "|       pip fruit|             yogurt| cream cheese |        meat spreads|\n",
            "|other vegetables|         whole milk|condensed milk|long life bakery ...|\n",
            "|      whole milk|             butter|        yogurt|                rice|\n",
            "|      rolls/buns|               null|          null|                null|\n",
            "|other vegetables|           UHT-milk|    rolls/buns|        bottled beer|\n",
            "|      pot plants|               null|          null|                null|\n",
            "|      whole milk|            cereals|          null|                null|\n",
            "+----------------+-------------------+--------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import requests\n",
        "from tempfile import NamedTemporaryFile\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Download the CSV file from GitHub\n",
        "url = 'https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/groceries.csv'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Write the response text to a temporary file\n",
        "with NamedTemporaryFile(mode='w', delete=False) as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = spark.read.csv(f.name)\n",
        "\n",
        "# Print the first 10 records\n",
        "df.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 2a: Solution"
      ],
      "metadata": {
        "id": "6DEH_Y314uCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the DataFrame to an RDD\n",
        "rdd = df.rdd\n",
        "\n",
        "# Extract the products from each transaction and remove duplicates\n",
        "products = rdd.flatMap(lambda x: x[0].split(',')).distinct()\n",
        "\n",
        "# Write the products to a text file\n",
        "with open('out/out_1_2a.txt', 'w') as f:\n",
        "    for product in products.collect():\n",
        "        f.write(product + '\\n')"
      ],
      "metadata": {
        "id": "rovt7YZx3GSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 2b: Solution"
      ],
      "metadata": {
        "id": "ShCam-065wFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the products from each transaction and remove duplicates\n",
        "products = rdd.flatMap(lambda x: x[0].split(',')).distinct()\n",
        "\n",
        "# Count the number of products\n",
        "count = products.count()\n",
        "\n",
        "# Write the count to a text file\n",
        "with open('out/out_1_2b.txt', 'w') as f:\n",
        "    f.write('Count:\\n')\n",
        "    f.write(str(count))"
      ],
      "metadata": {
        "id": "PKamWcDv4wVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 3 Solution"
      ],
      "metadata": {
        "id": "Qc5L0V4S7GyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of occurrences of each product\n",
        "product_counts = rdd.flatMap(lambda x: x[0].split(',')).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Sort the products by their frequency in descending order\n",
        "sorted_counts = product_counts.sortBy(lambda x: -x[1])\n",
        "\n",
        "# Take the top 5 products\n",
        "top_5 = sorted_counts.take(5)\n",
        "\n",
        "# Write the top 5 products and their frequencies to a text file\n",
        "with open('out/out_1_3.txt', 'w') as f:\n",
        "    for product, count in top_5:\n",
        "        f.write(\"('{}', {})\\n\".format(product, count))"
      ],
      "metadata": {
        "id": "rwU8HX957JKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 2: Spark Dataframe API "
      ],
      "metadata": {
        "id": "9pniY7Su8dHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 1 Solution"
      ],
      "metadata": {
        "id": "5joQIpG58bCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
        "# filename ='part-00000-tid-4320459746949313749-5c3d407c-c844-4016-97ad-2edec446aa62-6688-1-c000.snappy.parquet'\n",
        "# Read the parquet file into a DataFrame\n",
        "df = spark.read.parquet('/content/src/part-00000-tid-4320459746949313749-5c3d407c-c844-4016-97ad-2edec446aa62-6688-1-c000.snappy.parquet')\n",
        "\n",
        "# Print the schema of the DataFrame\n",
        "df.printSchema()\n",
        "\n",
        "# Show the first 10 rows of the DataFrame\n",
        "df.show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gbld1knR7yGZ",
        "outputId": "161ccbca-d093-4f4a-f034-b5177fe19a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- host_is_superhost: string (nullable = true)\n",
            " |-- cancellation_policy: string (nullable = true)\n",
            " |-- instant_bookable: string (nullable = true)\n",
            " |-- host_total_listings_count: double (nullable = true)\n",
            " |-- neighbourhood_cleansed: string (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- property_type: string (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- accommodates: double (nullable = true)\n",
            " |-- bathrooms: double (nullable = true)\n",
            " |-- bedrooms: double (nullable = true)\n",
            " |-- beds: double (nullable = true)\n",
            " |-- bed_type: string (nullable = true)\n",
            " |-- minimum_nights: double (nullable = true)\n",
            " |-- number_of_reviews: double (nullable = true)\n",
            " |-- review_scores_rating: double (nullable = true)\n",
            " |-- review_scores_accuracy: double (nullable = true)\n",
            " |-- review_scores_cleanliness: double (nullable = true)\n",
            " |-- review_scores_checkin: double (nullable = true)\n",
            " |-- review_scores_communication: double (nullable = true)\n",
            " |-- review_scores_location: double (nullable = true)\n",
            " |-- review_scores_value: double (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- bedrooms_na: double (nullable = true)\n",
            " |-- bathrooms_na: double (nullable = true)\n",
            " |-- beds_na: double (nullable = true)\n",
            " |-- review_scores_rating_na: double (nullable = true)\n",
            " |-- review_scores_accuracy_na: double (nullable = true)\n",
            " |-- review_scores_cleanliness_na: double (nullable = true)\n",
            " |-- review_scores_checkin_na: double (nullable = true)\n",
            " |-- review_scores_communication_na: double (nullable = true)\n",
            " |-- review_scores_location_na: double (nullable = true)\n",
            " |-- review_scores_value_na: double (nullable = true)\n",
            "\n",
            "+-----------------+--------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----+-----------+------------+-------+-----------------------+-------------------------+----------------------------+------------------------+------------------------------+-------------------------+----------------------+\n",
            "|host_is_superhost| cancellation_policy|instant_bookable|host_total_listings_count|neighbourhood_cleansed|latitude| longitude|property_type|      room_type|accommodates|bathrooms|bedrooms|beds|bed_type|minimum_nights|number_of_reviews|review_scores_rating|review_scores_accuracy|review_scores_cleanliness|review_scores_checkin|review_scores_communication|review_scores_location|review_scores_value|price|bedrooms_na|bathrooms_na|beds_na|review_scores_rating_na|review_scores_accuracy_na|review_scores_cleanliness_na|review_scores_checkin_na|review_scores_communication_na|review_scores_location_na|review_scores_value_na|\n",
            "+-----------------+--------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----+-----------+------------+-------+-----------------------+-------------------------+----------------------------+------------------------+------------------------------+-------------------------+----------------------+\n",
            "|                t|            moderate|               t|                      1.0|      Western Addition|37.76931|-122.43386|    Apartment|Entire home/apt|         3.0|      1.0|     1.0| 2.0|Real Bed|           1.0|            180.0|                97.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|               10.0|170.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "|                f|strict_14_with_gr...|               f|                      2.0|        Bernal Heights|37.74511|-122.42102|    Apartment|Entire home/apt|         5.0|      1.0|     2.0| 3.0|Real Bed|          30.0|            111.0|                98.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|                9.0|235.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "|                f|strict_14_with_gr...|               f|                     10.0|        Haight Ashbury|37.76669| -122.4525|    Apartment|   Private room|         2.0|      4.0|     1.0| 1.0|Real Bed|          32.0|             17.0|                85.0|                   8.0|                      8.0|                  9.0|                        9.0|                   9.0|                8.0| 65.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "|                f|strict_14_with_gr...|               f|                     10.0|        Haight Ashbury|37.76487|-122.45183|    Apartment|   Private room|         2.0|      4.0|     1.0| 1.0|Real Bed|          32.0|              8.0|                93.0|                   9.0|                      9.0|                 10.0|                       10.0|                   9.0|                9.0| 65.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "|                f|strict_14_with_gr...|               f|                      2.0|      Western Addition|37.77525|-122.43637|        House|Entire home/apt|         5.0|      1.5|     2.0| 2.0|Real Bed|           7.0|             27.0|                97.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|                9.0|785.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "|                f|            moderate|               f|                      1.0|      Western Addition|37.78471|-122.44555|    Apartment|Entire home/apt|         6.0|      1.0|     2.0| 3.0|Real Bed|           2.0|             31.0|                90.0|                   9.0|                      8.0|                 10.0|                       10.0|                   9.0|                9.0|255.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "|                t|strict_14_with_gr...|               t|                      2.0|               Mission|37.75919|-122.42237|  Condominium|   Private room|         3.0|      1.0|     1.0| 2.0|Real Bed|           1.0|            647.0|                98.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|               10.0|139.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "|                f|strict_14_with_gr...|               f|                      1.0|          Potrero Hill|37.76259|-122.40543|        House|   Private room|         2.0|      1.0|     1.0| 1.0|Real Bed|           1.0|            453.0|                94.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|               10.0|135.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "|                t|            moderate|               f|                      1.0|               Mission|37.75874|-122.41327|    Apartment|Entire home/apt|         6.0|      1.0|     2.0| 3.0|Real Bed|           3.0|            320.0|                96.0|                  10.0|                     10.0|                 10.0|                       10.0|                  10.0|                9.0|265.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "|                f|strict_14_with_gr...|               f|                     44.0|        Haight Ashbury|37.77187|-122.43859|    Apartment|Entire home/apt|         3.0|      1.0|     3.0| 3.0|Real Bed|          30.0|             37.0|                89.0|                   9.0|                      9.0|                 10.0|                        9.0|                   9.0|                9.0|177.0|        0.0|         0.0|    0.0|                    0.0|                      0.0|                         0.0|                     0.0|                           0.0|                      0.0|                   0.0|\n",
            "+-----------------+--------------------+----------------+-------------------------+----------------------+--------+----------+-------------+---------------+------------+---------+--------+----+--------+--------------+-----------------+--------------------+----------------------+-------------------------+---------------------+---------------------------+----------------------+-------------------+-----+-----------+------------+-------+-----------------------+-------------------------+----------------------------+------------------------+------------------------------+-------------------------+----------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 2 Solution\n"
      ],
      "metadata": {
        "id": "_cwJIaW--_v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Compute the minimum price, maximum price, and total row count\n",
        "min_price = df.select(F.min('price')).first()[0]\n",
        "max_price = df.select(F.max('price')).first()[0]\n",
        "row_count = df.select(F.count('*')).first()[0]\n",
        "\n",
        "# Create a DataFrame with the minimum price, maximum price, and total row count\n",
        "output_df = spark.createDataFrame([(min_price, max_price, row_count)], ['min_price', 'max_price', 'row_count'])\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "output_df.write.csv('out/out_2_2.txt', header=True)"
      ],
      "metadata": {
        "id": "HNeOcuC_9EMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 3 Solution"
      ],
      "metadata": {
        "id": "gHzvJlJ9AhYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Filter the DataFrame to only include properties with a price > 5000 and a review score of 10\n",
        "filtered_df = df.where((df.price > 5000) & (df.review_scores_rating == 10))\n",
        "\n",
        "# Compute the average number of bathrooms and bedrooms\n",
        "avg_bathrooms = filtered_df.select(F.avg('bathrooms')).first()[0]\n",
        "avg_bedrooms = filtered_df.select(F.avg('bedrooms')).first()[0]\n",
        "\n",
        "# Set the averages to 0 if they are None\n",
        "if avg_bathrooms is None:\n",
        "    avg_bathrooms = 0\n",
        "if avg_bedrooms is None:\n",
        "    avg_bedrooms = 0\n",
        "\n",
        "# Create a DataFrame with the average number of bathrooms and bedrooms\n",
        "output_df = spark.createDataFrame([(avg_bathrooms, avg_bedrooms)], ['avg_bathrooms', 'avg_bedrooms'])\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "output_df.write.csv('out/out_2_3.txt', header=True)"
      ],
      "metadata": {
        "id": "tPBhID3tABF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 4 Solution"
      ],
      "metadata": {
        "id": "FgD2ipqRBycD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Find the property with the lowest price\n",
        "lowest_price_df = df.where(df.price == df.select(F.min('price')).first()[0])\n",
        "\n",
        "# Find the property with the highest rating\n",
        "highest_rating_df = df.where(df.review_scores_rating == df.select(F.max('review_scores_rating')).first()[0])\n",
        "\n",
        "# Find the intersection of the two DataFrames\n",
        "intersection_df = lowest_price_df.intersect(highest_rating_df)\n",
        "\n",
        "# Get the number of people that can be accommodated by the property\n",
        "people_accommodated = intersection_df.first()['accommodates']\n",
        "\n",
        "# Write the number of people that can be accommodated to a text file\n",
        "with open('out/out_2_4.txt', 'w') as f:\n",
        "    f.write(str(people_accommodated))\n"
      ],
      "metadata": {
        "id": "DbPdQ-urAtUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 5 Solution"
      ],
      "metadata": {
        "id": "3_uU7V5qCG8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/task_2_5.py\n",
        "\n",
        "# Import the required libraries\n",
        "from airflow import DAG\n",
        "from airflow.operators.dummy_operator import DummyOperator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set default_args dictionary to specify default parameters of the DAG, such as the start date and frequency of runs\n",
        "default_args = {\n",
        "    'owner': 'me',\n",
        "    'start_date': datetime(2023, 1, 1),\n",
        "    'depends_on_past': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "# Create a DAG instance and pass it the default_args dictionary\n",
        "dag = DAG(\n",
        "    'task_2_5',\n",
        "    default_args=default_args,\n",
        "    schedule_interval=timedelta(days=1)\n",
        ")\n",
        "\n",
        "# Create DummyOperator instances for each task\n",
        "task1 = DummyOperator(task_id='task_1', dag=dag)\n",
        "task2 = DummyOperator(task_id='task_2', dag=dag)\n",
        "task3 = DummyOperator(task_id='task_3', dag=dag)\n",
        "task4 = DummyOperator(task_id='task_4', dag=dag)\n",
        "task5 = DummyOperator(task_id='task_5', dag=dag)\n",
        "task6 = DummyOperator(task_id='task_6', dag=dag)\n",
        "\n",
        "# Set task dependencies\n",
        "task1 >> [task2, task3] >> [task4, task5, task6]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjZidDEgJmH-",
        "outputId": "f5b4a49e-c3c3-4776-fb25-29154fc53ef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/task_2_5.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3: Applied Machine Learning\n"
      ],
      "metadata": {
        "id": "3Sf7c8gGRhlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 1 Solution"
      ],
      "metadata": {
        "id": "HNmnx8zo8wwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Machine Learning Code"
      ],
      "metadata": {
        "id": "lOGV_AWV83j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" -o /tmp/iris.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhog7aO9T5QX",
        "outputId": "9fc5c120-5c0b-4cc7-d09a-c6835f19d806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4551  100  4551    0     0  39573      0 --:--:-- --:--:-- --:--:-- 39573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.linear_model import LogisticRegression \n",
        "df = pd.read_csv(\"/tmp/iris.csv\", \n",
        "names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]) \n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "56ZFlYbBT--P",
        "outputId": "28e3c573-725a-4504-fe2d-f3726b21379f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width        class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6abf6281-2757-4d64-8751-274f2154b49d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6abf6281-2757-4d64-8751-274f2154b49d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6abf6281-2757-4d64-8751-274f2154b49d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6abf6281-2757-4d64-8751-274f2154b49d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features from class. \n",
        "array = df.values \n",
        "X = array[:,0:4] \n",
        "y = array[:,4] \n",
        "# Fit Logistic Regression classifier. \n",
        "logreg = LogisticRegression(C=1e5) \n",
        "logreg.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiWln5LIUShs",
        "outputId": "890c7fcd-5e8c-407c-8b51-a2867cf6f54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on training data. Seems to work. \n",
        "# 5.1 3.5 1.4 0.2 Iris-setosa \n",
        "# 6.2 3.4 5.4 2.3 Iris-virginica \n",
        "print(logreg.predict([[5.1, 3.5, 1.4, 0.2]])) \n",
        "print(logreg.predict([[6.2, 3.4, 5.4, 2.3]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fycTM84UZ6o",
        "outputId": "95795e7e-168a-46bc-c0f7-5e3c87d82b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Iris-setosa']\n",
            "['Iris-virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark Code"
      ],
      "metadata": {
        "id": "OJv1z3WhnK5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Classifier\").getOrCreate()\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
        "\n",
        "# Define the schema for the DataFrame\n",
        "schema = StructType([\n",
        "    StructField(\"sepal_length\", DoubleType(), True),\n",
        "    StructField(\"sepal_width\", DoubleType(), True),\n",
        "    StructField(\"petal_length\", DoubleType(), True),\n",
        "    StructField(\"petal_width\", DoubleType(), True),\n",
        "    StructField(\"class\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Load the data with the specified schema\n",
        "df = spark.read.csv(\"/tmp/iris.csv\", header=False, schema=schema)\n",
        "df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsV-DbHCe14A",
        "outputId": "c0d83279-a83e-44a4-bedc-cb7d13b10f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|      class|\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
            "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n",
            "|         4.6|        3.4|         1.4|        0.3|Iris-setosa|\n",
            "|         5.0|        3.4|         1.5|        0.2|Iris-setosa|\n",
            "|         4.4|        2.9|         1.4|        0.2|Iris-setosa|\n",
            "|         4.9|        3.1|         1.5|        0.1|Iris-setosa|\n",
            "|         5.4|        3.7|         1.5|        0.2|Iris-setosa|\n",
            "|         4.8|        3.4|         1.6|        0.2|Iris-setosa|\n",
            "|         4.8|        3.0|         1.4|        0.1|Iris-setosa|\n",
            "|         4.3|        3.0|         1.1|        0.1|Iris-setosa|\n",
            "|         5.8|        4.0|         1.2|        0.2|Iris-setosa|\n",
            "|         5.7|        4.4|         1.5|        0.4|Iris-setosa|\n",
            "|         5.4|        3.9|         1.3|        0.4|Iris-setosa|\n",
            "|         5.1|        3.5|         1.4|        0.3|Iris-setosa|\n",
            "|         5.7|        3.8|         1.7|        0.3|Iris-setosa|\n",
            "|         5.1|        3.8|         1.5|        0.3|Iris-setosa|\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Convert the string column \"class\" to a numerical column\n",
        "stringIndexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
        "model = stringIndexer.fit(df)\n",
        "indexed = model.transform(df)\n",
        "\n",
        "# Create a feature vector by combining all of the columns\n",
        "assembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
        "\n",
        "# Transform the DataFrame to a features DataFrame\n",
        "features = assembler.transform(indexed)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "training, test = features.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Create the logistic regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"classIndex\", maxIter=10)\n",
        "\n",
        "# Fit the model to the training data\n",
        "lrModel = lr.fit(training)\n"
      ],
      "metadata": {
        "id": "qTie8l4VdVKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "# Create a dictionary to map numerical predictions to class names\n",
        "class_mapping = {0: \"Iris-setosa\", 1: \"Iris-versicolor\", 2: \"Iris-virginica\"}\n",
        "cl1_prediction = lrModel.predict(Vectors.dense([5.1, 3.5, 1.4, 0.2]))\n",
        "cl2_prediction = lrModel.predict(Vectors.dense([6.2, 3.4, 5.4, 2.3]))\n",
        "\n",
        "# Convert the numerical prediction to a class name\n",
        "pred_cl1 = class_mapping[cl1_prediction]\n",
        "pred_cl2 = class_mapping[cl2_prediction]\n",
        "print(pred_cl1)\n",
        "print(pred_cl2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8jKWdGEk6XU",
        "outputId": "b3f0cda0-f7dd-4fbf-bcaa-56b77c182f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris-setosa\n",
            "Iris-virginica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 2 Solution"
      ],
      "metadata": {
        "id": "7TczrewFRsMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Convert the string column \"class\" to a numerical column\n",
        "stringIndexer = StringIndexer(inputCol=\"class\", outputCol=\"classIndex\")\n",
        "model = stringIndexer.fit(df)\n",
        "indexed = model.transform(df)\n",
        "\n",
        "# Create a feature vector by combining all of the columns\n",
        "assembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
        "\n",
        "# Transform the DataFrame to a features DataFrame\n",
        "features = assembler.transform(indexed)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "training, test = features.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Create the logistic regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"classIndex\", maxIter=10)\n",
        "\n",
        "# Fit the model to the training data\n",
        "lrModel = lr.fit(training)\n",
        "\n",
        "# Create a dataframe for the predictions\n",
        "pred_data = spark.createDataFrame( \n",
        " [(5.1, 3.5, 1.4, 0.2),  \n",
        " (6.2, 3.4, 5.4, 2.3)], \n",
        " [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]) \n",
        "\n",
        "# Apply the VectorAssembler to pred_data to create the \"features\" column\n",
        "pred_features = assembler.transform(pred_data)\n",
        "\n",
        "# Use the transform method of the lrModel object to generate predictions for pred_features\n",
        "predictions = lrModel.transform(pred_features)\n",
        "\n",
        "# Show the predictions\n",
        "predictions.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ5F_bhwb_5w",
        "outputId": "4a8f7543-541d-4209-e0e5-e1a229e341a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------------+--------------------+--------------------+----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|         features|       rawPrediction|         probability|prediction|\n",
            "+------------+-----------+------------+-----------+-----------------+--------------------+--------------------+----------+\n",
            "|         5.1|        3.5|         1.4|        0.2|[5.1,3.5,1.4,0.2]|[17.1574759755653...|[0.99989880750343...|       0.0|\n",
            "|         6.2|        3.4|         5.4|        2.3|[6.2,3.4,5.4,2.3]|[-12.742254221498...|[1.58323522338701...|       2.0|\n",
            "+------------+-----------+------------+-----------+-----------------+--------------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the predictions to class names\n",
        "predictions = predictions.rdd.map(lambda x: (x[\"prediction\"], class_mapping[x[\"prediction\"]])).toDF([\"prediction\", \"class\"])\n",
        "\n",
        "# Write the predictions to a CSV file\n",
        "predictions.write.csv(\"out/out_3_2.txt\", header=True, mode=\"overwrite\")\n"
      ],
      "metadata": {
        "id": "MHp5T-FodTSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}